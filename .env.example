# Ollama Configuration
OLLAMA_HOST=http://localhost
OLLAMA_PORT=11434
OLLAMA_MODEL=llama2

# Model Configuration
MODEL=llama2

# Optional: Ollama API Key (if using cloud version)
# OLLAMA_API_KEY=your_api_key_here

# Optional: Gemini API Key (for legacy Python version)
# GEMINI_API_KEY=your_api_key_here

# Rust Configuration
RUST_BACKTRACE=1