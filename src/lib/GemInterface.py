import os
import asyncio
from dotenv import load_dotenv
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from typing import Any,  AsyncIterator
import json
import sys
from ollama import AsyncClient, web_fetch, web_search
import inspect
import datetime
class AiInterface:
    """
    AI Interface using Ollama for local LLM inference with streaming support.
    Keeps the original synchronous web scraper (requests + BeautifulSoup) but improves it
    by adding a requests.Session with browser-like headers and a retry strategy to reduce
    403/429/5xx failures. Everything else remains async-friendly by running blocking
    operations in a threadpool.

    Usage:
      ai = AiInterface()
      result = asyncio.run(ai.Archie("When is fall break?"))
    """

    def __init__(
        
        self,
        
        debug: bool = False,
        scraper_max_retries: int = 3,
        scraper_backoff_factor: float = 1.0,
        scraper_timeout: int = 15,
        available_tools = {'web_search': web_search, 'web_fetch': web_fetch}
    ):
        # Load the variables from the .env file into the environment
        load_dotenv()

        # Retrieve the model name from environment (defaults to llama2 if not set)
        self.model = os.getenv("MODEL", "llama2")

        # Debug flag
        self.debug = debug

        # Scraper configuration
        self.scraper_timeout = scraper_timeout

        # Create a requests.Session with headers and retry strategy
        self.session: requests.Session = requests.Session()
        # Browser-like default headers to reduce likelihood of 403 from simple bot checks
        self.session.headers.update({
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
        })

        # Configure retries for transient errors and common rate-limit statuses.
        retry_strategy = Retry(
            total=scraper_max_retries,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"],
            backoff_factor=scraper_backoff_factor,
            raise_on_status=False,
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)

    def _log(self, *args):
        if self.debug:
            print("[AiInterface DEBUG]", *args)



    #I dont think this is used anywhere but im keeping it just in case

    async def generate_text_streaming(self, prompt: str, system_prompt: str = "") -> AsyncIterator[str]:
        """
        Async streaming generator that yields tokens as they are generated by Ollama.
        This allows for real-time display of the AI's thinking process.
        
        Usage:
            async for token in ai.generate_text_streaming(prompt, system):
                print(token, end='', flush=True)
        """
        messages = []
        if system_prompt:
            messages.append({
                'role': 'system',
                'content': system_prompt,
            })
        messages.append({
            'role': 'user',
            'content': prompt,
        })
        
        
        # Create a new AsyncClient for each streaming request to avoid event loop conflicts
        async_client = AsyncClient()
        stream = await async_client.chat(
            model=self.model,
            messages=messages,
            stream=True,

        
        )

        async for chunk in stream:
            if 'message' in chunk and 'content' in chunk['message']:
                yield chunk['message']['content']
       
    
    #I dont think this is used anywhere but im keeping it just in case
    
    async def Archie(self, query: str, conversation_history: list = None) -> str:
        """
        Main async entry point for the Archie AI assistant.
        Uses scraped data from JSON file to provide context for answering queries.
        Uses Ollama tool calling to enable web search when needed.
        """
        with open("data/scrape_results.json", "r", encoding="utf-8") as f:
            results = json.load(f)
        
        # Build messages list with system prompt and conversation history
        messages = []
        
        system_content = f"""You are ArchieAI, an AI assistant for Arcadia University. You are here to help students, faculty, and staff with any questions they may have about the university.

You are made by students for a final project. You must be factual and accurate based on the information provided.

Respond based on your knowledge up to 2025.

Use the following university data to answer questions:
{json.dumps(results, indent=2)}

If the university data doesn't contain the information needed, or if the query requires current/real-time information, you can use the search_web tool to find additional information."""
        
        messages.append({
            'role': 'system',
            'content': system_content
        })
        
        # Add conversation history
        if conversation_history:
            for msg in conversation_history[-5:]:  # Last 5 messages for context
                messages.append({
                    'role': msg.get('role', 'user'),
                    'content': msg.get('content', '')
                })
        
        # Add current query
        messages.append({
            'role': 'user',
            'content': query
        })
        
        # Call with tools - run in executor since it's synchronous

    async def async_WebSearch(self, prompt: str, system_prompt: str = "", available_tools = {'web_search': web_search, 'web_fetch': web_fetch}) -> AsyncIterator[Any]:
        
            
        """
        Async generator that yields streamed content chunks as they arrive.
        Yields:
        - str: incremental content chunks from the assistant
        - dict: tool call results in the form {'tool_name': ..., 'tool_result': ...}
        - dict: final message when done: {'final': True, 'message': final_response_message}
        """
        OLLAMA_API_KEY = os.getenv('OLLAMA_API_KEY') or os.getenv('OLLAMA_TOKEN')
        if not OLLAMA_API_KEY:
            print("Error: OLLAMA_API_KEY (or OLLAMA_TOKEN) not found in environment; add it to your .env or export it before running.")
            sys.exit(1)
        MODEL = os.getenv('OLLAMA_MODEL')

        # Normalize to OLLAMA_API_KEY for the Ollama client if the token was provided under OLLAMA_TOKEN.
        # This took me way too long to figure out Headers are of the devil and there is no documentation on this.
        custom_headers = {
            "Authorization": f"Bearer {OLLAMA_API_KEY}"
        }
        client = AsyncClient(headers=custom_headers)
        messages = [{'role': 'user', 'content': prompt}, {'role': 'system', 'content': system_prompt}]
        while True:
            response_stream = await client.chat(
                model=MODEL,
                messages=messages,
                tools=[client.web_search, client.web_fetch],
                think=True,
                stream=True
            )

            final_response_message = {
                'role': 'assistant',
                'content': '',
                'thinking': None,
                'tool_calls': None
            }

            # Iterate asynchronously through streamed chunks and yield content as it arrives
            async for response_chunk in response_stream:
                chunk_message = response_chunk.message

                if chunk_message.thinking:
                    if not final_response_message['thinking']:
                        final_response_message['thinking'] = chunk_message.thinking

                if chunk_message.content:
                    final_response_message['content'] += chunk_message.content
                    # yield incremental content chunk
                    yield chunk_message.content

                if chunk_message.tool_calls:
                    final_response_message['tool_calls'] = chunk_message.tool_calls

            # Add the assistant's final streamed message into the conversation history
            messages.append(final_response_message)

            # If the model requested tools, execute them and yield their results, then continue the loop
            if final_response_message['tool_calls']:
                tool_calls = final_response_message['tool_calls']

                for tool_call in tool_calls:
                    tool_name = tool_call.function.name
                    function_to_call = available_tools.get(tool_name)

                    if function_to_call:
                        args = getattr(tool_call.function, 'arguments', {}) or {}

                        if inspect.iscoroutinefunction(function_to_call):
                            result = await function_to_call(**args)
                        else:
                            maybe_result = function_to_call(**args)
                            if inspect.isawaitable(maybe_result):
                                result = await maybe_result
                            else:
                                result = maybe_result

                        # Append tool result to messages so the model can continue the conversation
                        messages.append({
                            'role': 'tool',
                            'content': str(result)[:2000 * 4],
                            'tool_name': tool_name
                        })

                        # Yield the tool result to the caller
                        yield {'tool_name': tool_name, 'tool_result': result}
                    else:
                        messages.append({
                            'role': 'tool',
                            'content': f'Tool {tool_name} not found',
                            'tool_name': tool_name
                        })
                        yield {'tool_name': tool_name, 'tool_result': None, 'error': 'tool_not_found'}
                # continue to next iteration so the model can respond to tool results
            else:
                # No tool calls: streaming finished; yield final assembled message and exit
                yield {'final': True, 'message': final_response_message}
                break
    
    async def Archie_streaming(self, query: str, conversation_history: list = None) -> AsyncIterator[str]:
        """
        Streaming version of Archie that yields tokens as they are generated.
        Note: Tool calling with streaming is complex, so this version uses the standard approach.
        For full tool calling support, use the non-streaming Archie() method.
        
        Usage:
            async for token in ai.Archie_streaming("When is fall break?"):
                print(token, end='', flush=True)
        """
        
        # Build context with conversation history
        history_context = ""
        if conversation_history:
            history_context = "\n\nConversation History:\n"
            for msg in conversation_history: 
                role = msg.get("role", "user")
                content = msg.get("content", "")
                history_context += f"{role.upper()}: {content}\n"

        system_prompt = f"""You are ArchieAI, an AI assistant for Arcadia University IN glenside pennsylvania. Do not mention Georgia or the arcadia university in georgia. You are here to help students, faculty, and staff with any questions they may have about the university.

You are made by students for a final project. You must be factual and concise based on the information provided. All responses should be professional yet to the point.
Markdown IS NOT SUPPORTED OR RENDERED in the final output. DO NOT RESPOND WITH MARKDOWN FORMATTING OR HYPERLINKS so no [links](url) formatting or bolding. however you can provide full URLs.
You are not associated with Arcadia University officially as you are a student project.
History:
{history_context}
The Time is {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""     

        async for token in self.async_WebSearch(query, system_prompt=system_prompt):
            yield token
    
