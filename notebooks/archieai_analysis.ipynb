{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArchieAI User Interaction Analysis\n",
    "## CS222 Final Project - Fall 2025\n",
    "\n",
    "**Author:** Eva Akselrad\n",
    "\n",
    "**Project Title:** Understanding Student Engagement Patterns with an AI Assistant\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Data Loading and Preparation](#data-loading)\n",
    "3. [Data Cleaning and Preprocessing](#preprocessing)\n",
    "4. [Exploratory Data Analysis](#eda)\n",
    "5. [Feature Engineering](#feature-engineering)\n",
    "6. [Machine Learning Model Development](#ml-model)\n",
    "7. [Model Evaluation](#evaluation)\n",
    "8. [Results and Insights](#results)\n",
    "9. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook analyzes user interaction patterns with ArchieAI, an AI-powered assistant for Arcadia University students.\n",
    "\n",
    "**Research Questions:**\n",
    "1. What are the common usage patterns (time of day, session duration, message counts)?\n",
    "2. How do users engage with the AI assistant?\n",
    "3. Can we predict user engagement level based on interaction features?\n",
    "\n",
    "**Methodology:**\n",
    "- Exploratory Data Analysis (EDA) with visualizations\n",
    "- Feature engineering from session logs\n",
    "- Machine learning classification to predict engagement levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data-loading'></a>\n",
    "## 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Visualization settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_DIR = '../data'\n",
    "SESSIONS_DIR = os.path.join(DATA_DIR, 'sessions')\n",
    "DATASETS_DIR = os.path.join(DATA_DIR, 'datasets')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(DATASETS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Sessions directory: {SESSIONS_DIR}\")\n",
    "print(f\"Datasets directory: {DATASETS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session_data(sessions_dir):\n",
    "    \"\"\"\n",
    "    Load all session JSON files and extract relevant information.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sessions_dir : str\n",
    "        Path to the directory containing session JSON files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing processed session data\n",
    "    \"\"\"\n",
    "    sessions_data = []\n",
    "    \n",
    "    # Check if sessions directory exists\n",
    "    if not os.path.exists(sessions_dir):\n",
    "        print(f\"Warning: Sessions directory '{sessions_dir}' does not exist.\")\n",
    "        print(\"Creating sample data for demonstration purposes...\")\n",
    "        return create_sample_data()\n",
    "    \n",
    "    # Load all session files\n",
    "    session_files = [f for f in os.listdir(sessions_dir) if f.endswith('.json')]\n",
    "    \n",
    "    if len(session_files) == 0:\n",
    "        print(\"No session files found. Creating sample data for demonstration...\")\n",
    "        return create_sample_data()\n",
    "    \n",
    "    print(f\"Found {len(session_files)} session files.\")\n",
    "    \n",
    "    for filename in session_files:\n",
    "        filepath = os.path.join(sessions_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                session = json.load(f)\n",
    "                \n",
    "                # Extract session information\n",
    "                session_info = {\n",
    "                    'session_id': session.get('id', filename.replace('.json', '')),\n",
    "                    'user_id': session.get('user_id', 'guest'),\n",
    "                    'created_at': session.get('created_at'),\n",
    "                    'last_activity': session.get('last_activity'),\n",
    "                    'message_count': len(session.get('messages', [])),\n",
    "                    'messages': session.get('messages', [])\n",
    "                }\n",
    "                sessions_data.append(session_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    if len(sessions_data) == 0:\n",
    "        print(\"No valid session data found. Creating sample data...\")\n",
    "        return create_sample_data()\n",
    "    \n",
    "    return pd.DataFrame(sessions_data)\n",
    "\n",
    "\n",
    "def create_sample_data(n_samples=100):\n",
    "    \"\"\"\n",
    "    Create sample session data for demonstration purposes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of sample sessions to create\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing sample session data\n",
    "    \"\"\"\n",
    "    print(f\"Generating {n_samples} sample sessions...\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate sample data\n",
    "    base_date = datetime.now() - timedelta(days=30)\n",
    "    \n",
    "    sessions_data = []\n",
    "    for i in range(n_samples):\n",
    "        # Random session timing\n",
    "        days_offset = int(np.random.randint(0, 30))\n",
    "        hour = int(np.random.choice([9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21], \n",
    "                                p=[0.05, 0.08, 0.1, 0.08, 0.12, 0.15, 0.12, 0.1, 0.08, 0.06, 0.04, 0.02]))\n",
    "        minute = int(np.random.randint(0, 60))\n",
    "        \n",
    "        created_at = base_date + timedelta(days=days_offset, hours=hour, minutes=minute)\n",
    "        \n",
    "        # Random session duration (5-60 minutes)\n",
    "        duration_minutes = np.random.gamma(shape=2, scale=10)\n",
    "        duration_minutes = float(min(max(duration_minutes, 5), 60))\n",
    "        \n",
    "        last_activity = created_at + timedelta(minutes=duration_minutes)\n",
    "        \n",
    "        # Random message count (1-30 messages)\n",
    "        message_count = int(max(1, int(np.random.gamma(shape=2, scale=3))))\n",
    "        \n",
    "        # Generate sample messages\n",
    "        messages = []\n",
    "        for j in range(message_count):\n",
    "            time_offset = (duration_minutes / message_count) * j\n",
    "            msg_time = created_at + timedelta(minutes=time_offset)\n",
    "            \n",
    "            if j % 2 == 0:  # User message\n",
    "                messages.append({\n",
    "                    'role': 'user',\n",
    "                    'content': f'Sample question {j//2 + 1}',\n",
    "                    'timestamp': msg_time.isoformat()\n",
    "                })\n",
    "            else:  # Assistant message\n",
    "                messages.append({\n",
    "                    'role': 'assistant',\n",
    "                    'content': f'Sample response {j//2 + 1}',\n",
    "                    'timestamp': msg_time.isoformat()\n",
    "                })\n",
    "        \n",
    "        session_info = {\n",
    "            'session_id': f'sample_session_{i+1}',\n",
    "            'user_id': f'user_{int(np.random.randint(1, 20))}' if np.random.random() > 0.3 else 'guest',\n",
    "            'created_at': created_at.isoformat(),\n",
    "            'last_activity': last_activity.isoformat(),\n",
    "            'message_count': message_count,\n",
    "            'messages': messages\n",
    "        }\n",
    "        sessions_data.append(session_info)\n",
    "    \n",
    "    return pd.DataFrame(sessions_data)\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df_raw = load_session_data(SESSIONS_DIR)\n",
    "print(f\"\\nLoaded {len(df_raw)} sessions.\")\n",
    "print(f\"\\nDataset shape: {df_raw.shape}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocessing'></a>\n",
    "## 3. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df_raw.isnull().sum())\n",
    "print(\"\\nData types:\")\n",
    "print(df_raw.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp strings to datetime objects\n",
    "df_raw['created_at'] = pd.to_datetime(df_raw['created_at'])\n",
    "df_raw['last_activity'] = pd.to_datetime(df_raw['last_activity'])\n",
    "\n",
    "# Calculate session duration in minutes\n",
    "df_raw['duration_minutes'] = (df_raw['last_activity'] - df_raw['created_at']).dt.total_seconds() / 60\n",
    "\n",
    "# Extract temporal features\n",
    "df_raw['hour'] = df_raw['created_at'].dt.hour\n",
    "df_raw['day_of_week'] = df_raw['created_at'].dt.dayofweek\n",
    "df_raw['day_name'] = df_raw['created_at'].dt.day_name()\n",
    "df_raw['date'] = df_raw['created_at'].dt.date\n",
    "\n",
    "# Create time of day categories\n",
    "def categorize_time_of_day(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_raw['time_of_day'] = df_raw['hour'].apply(categorize_time_of_day)\n",
    "\n",
    "# Identify if user is guest or registered\n",
    "df_raw['is_guest'] = df_raw['user_id'] == 'guest'\n",
    "\n",
    "print(\"Temporal features extracted successfully!\")\n",
    "print(f\"\\nUpdated dataset shape: {df_raw.shape}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal Sessions: {len(df_raw)}\")\n",
    "print(f\"Date Range: {df_raw['created_at'].min()} to {df_raw['created_at'].max()}\")\n",
    "print(f\"Unique Users: {df_raw['user_id'].nunique()}\")\n",
    "print(f\"Guest Sessions: {df_raw['is_guest'].sum()} ({df_raw['is_guest'].sum()/len(df_raw)*100:.1f}%)\")\n",
    "print(f\"\\nSession Duration (minutes):\")\n",
    "print(df_raw['duration_minutes'].describe())\n",
    "print(f\"\\nMessage Count per Session:\")\n",
    "print(df_raw['message_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Session duration distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Duration histogram\n",
    "axes[0].hist(df_raw['duration_minutes'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Session Duration (minutes)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Session Durations')\n",
    "axes[0].axvline(df_raw['duration_minutes'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_raw[\"duration_minutes\"].mean():.1f} min')\n",
    "axes[0].legend()\n",
    "\n",
    "# Message count histogram\n",
    "axes[1].hist(df_raw['message_count'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Number of Messages')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Messages per Session')\n",
    "axes[1].axvline(df_raw['message_count'].mean(), color='red', linestyle='--',\n",
    "                label=f'Mean: {df_raw[\"message_count\"].mean():.1f} messages')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATASETS_DIR, 'distributions.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Usage by time of day\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Sessions by hour\n",
    "hour_counts = df_raw['hour'].value_counts().sort_index()\n",
    "axes[0].bar(hour_counts.index, hour_counts.values, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Hour of Day')\n",
    "axes[0].set_ylabel('Number of Sessions')\n",
    "axes[0].set_title('Session Distribution by Hour of Day')\n",
    "axes[0].set_xticks(range(0, 24))\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Sessions by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_counts = df_raw['day_name'].value_counts().reindex(day_order)\n",
    "axes[1].bar(range(len(day_counts)), day_counts.values, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].set_xlabel('Day of Week')\n",
    "axes[1].set_ylabel('Number of Sessions')\n",
    "axes[1].set_title('Session Distribution by Day of Week')\n",
    "axes[1].set_xticks(range(len(day_order)))\n",
    "axes[1].set_xticklabels(day_order, rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATASETS_DIR, 'temporal_patterns.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Time of day analysis\n",
    "time_of_day_counts = df_raw['time_of_day'].value_counts()\n",
    "time_order = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "time_of_day_counts = time_of_day_counts.reindex(time_order)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar chart\n",
    "axes[0].bar(range(len(time_of_day_counts)), time_of_day_counts.values, \n",
    "            edgecolor='black', alpha=0.7, color=['#FFD700', '#FF8C00', '#FF4500', '#4B0082'])\n",
    "axes[0].set_xlabel('Time of Day')\n",
    "axes[0].set_ylabel('Number of Sessions')\n",
    "axes[0].set_title('Session Distribution by Time of Day')\n",
    "axes[0].set_xticks(range(len(time_order)))\n",
    "axes[0].set_xticklabels(time_order)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(time_of_day_counts.values, labels=time_of_day_counts.index, autopct='%1.1f%%',\n",
    "            colors=['#FFD700', '#FF8C00', '#FF4500', '#4B0082'], startangle=90)\n",
    "axes[1].set_title('Proportion of Sessions by Time of Day')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATASETS_DIR, 'time_of_day.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Correlation analysis\n",
    "numeric_cols = ['duration_minutes', 'message_count', 'hour', 'day_of_week']\n",
    "correlation_matrix = df_raw[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATASETS_DIR, 'correlation_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Correlations:\")\n",
    "print(f\"Duration vs Message Count: {correlation_matrix.loc['duration_minutes', 'message_count']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature-engineering'></a>\n",
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engagement level based on session characteristics\n",
    "def classify_engagement(row):\n",
    "    \"\"\"\n",
    "    Classify engagement level based on duration and message count.\n",
    "    \n",
    "    Criteria:\n",
    "    - High: duration > 20 min OR message_count > 10\n",
    "    - Low: duration < 10 min AND message_count < 5\n",
    "    - Medium: Everything else\n",
    "    \"\"\"\n",
    "    if row['duration_minutes'] > 20 or row['message_count'] > 10:\n",
    "        return 'High'\n",
    "    elif row['duration_minutes'] < 10 and row['message_count'] < 5:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "\n",
    "df_raw['engagement_level'] = df_raw.apply(classify_engagement, axis=1)\n",
    "\n",
    "# Display engagement distribution\n",
    "print(\"Engagement Level Distribution:\")\n",
    "print(df_raw['engagement_level'].value_counts())\n",
    "print(\"\\nPercentages:\")\n",
    "print(df_raw['engagement_level'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Engagement levels\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar chart\n",
    "engagement_counts = df_raw['engagement_level'].value_counts()\n",
    "colors_map = {'High': '#2E7D32', 'Medium': '#FFA726', 'Low': '#D32F2F'}\n",
    "colors = [colors_map[level] for level in engagement_counts.index]\n",
    "\n",
    "axes[0].bar(range(len(engagement_counts)), engagement_counts.values, \n",
    "            edgecolor='black', alpha=0.7, color=colors)\n",
    "axes[0].set_xlabel('Engagement Level')\n",
    "axes[0].set_ylabel('Number of Sessions')\n",
    "axes[0].set_title('Distribution of Engagement Levels')\n",
    "axes[0].set_xticks(range(len(engagement_counts)))\n",
    "axes[0].set_xticklabels(engagement_counts.index)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot: Duration by engagement level\n",
    "engagement_order = ['Low', 'Medium', 'High']\n",
    "df_raw['engagement_level'] = pd.Categorical(df_raw['engagement_level'], \n",
    "                                             categories=engagement_order, ordered=True)\n",
    "sns.boxplot(data=df_raw, x='engagement_level', y='duration_minutes', ax=axes[1],\n",
    "            palette={'Low': '#D32F2F', 'Medium': '#FFA726', 'High': '#2E7D32'})\n",
    "axes[1].set_xlabel('Engagement Level')\n",
    "axes[1].set_ylabel('Session Duration (minutes)')\n",
    "axes[1].set_title('Session Duration by Engagement Level')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATASETS_DIR, 'engagement_levels.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for machine learning\n",
    "# Select features for modeling\n",
    "feature_cols = ['message_count', 'hour', 'day_of_week', 'is_guest']\n",
    "target_col = 'engagement_level'\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = df_raw[feature_cols].copy()\n",
    "y = df_raw[target_col].copy()\n",
    "\n",
    "# Convert boolean to int\n",
    "X['is_guest'] = X['is_guest'].astype(int)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "print(\"\\nFeatures:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml-model'></a>\n",
    "## 6. Machine Learning Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                      random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(\"\\nTraining set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTesting set target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
    "    'Precision': [results[m]['precision'] for m in results.keys()],\n",
    "    'Recall': [results[m]['recall'] for m in results.keys()],\n",
    "    'F1-Score': [results[m]['f1'] for m in results.keys()],\n",
    "    'CV Mean': [results[m]['cv_mean'] for m in results.keys()],\n",
    "    'CV Std': [results[m]['cv_std'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']\n",
    "print(f\"Best Model (by Accuracy): {best_model_name}\")\n",
    "print(f\"Best Accuracy: {comparison_df['Accuracy'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, model_name in enumerate(results.keys()):\n",
    "    values = [results[model_name][m.lower().replace('-', '')] for m in metrics]\n",
    "    axes[0].bar(x + i*width, values, width, label=model_name, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Metrics')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Model Performance Comparison')\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_means = [results[m]['cv_mean'] for m in results.keys()]\n",
    "cv_stds = [results[m]['cv_std'] for m in results.keys()]\n",
    "axes[1].bar(range(len(results)), cv_means, yerr=cv_stds, \n",
    "            capsize=5, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Models')\n",
    "axes[1].set_ylabel('Cross-Validation Score')\n",
    "axes[1].set_title('Cross-Validation Performance')\n",
    "axes[1].set_xticks(range(len(results)))\n",
    "axes[1].set_xticklabels(results.keys(), rotation=15)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_ylim([0, 1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATASETS_DIR, 'model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of best model\n",
    "best_model = results[best_model_name]['model']\n",
    "y_pred_best = results[best_model_name]['predictions']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"DETAILED EVALUATION: {best_model_name}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best, labels=['Low', 'Medium', 'High'])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Low', 'Medium', 'High'],\n",
    "            yticklabels=['Low', 'Medium', 'High'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'Confusion Matrix: {best_model_name}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATASETS_DIR, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for tree-based models)\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], \n",
    "             edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(f'Feature Importance: {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(DATASETS_DIR, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='results'></a>\n",
    "## 8. Results and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"=\" * 80)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   - Total sessions analyzed: {len(df_raw)}\")\n",
    "print(f\"   - Average session duration: {df_raw['duration_minutes'].mean():.1f} minutes\")\n",
    "print(f\"   - Average messages per session: {df_raw['message_count'].mean():.1f}\")\n",
    "print(f\"   - Most common time of day: {df_raw['time_of_day'].mode()[0]}\")\n",
    "print(f\"   - Most active day: {df_raw['day_name'].mode()[0]}\")\n",
    "\n",
    "print(\"\\n2. ENGAGEMENT PATTERNS:\")\n",
    "engagement_pct = df_raw['engagement_level'].value_counts(normalize=True) * 100\n",
    "for level in ['High', 'Medium', 'Low']:\n",
    "    if level in engagement_pct.index:\n",
    "        print(f\"   - {level} engagement: {engagement_pct[level]:.1f}%\")\n",
    "\n",
    "print(\"\\n3. MACHINE LEARNING RESULTS:\")\n",
    "print(f\"   - Best model: {best_model_name}\")\n",
    "print(f\"   - Test accuracy: {results[best_model_name]['accuracy']:.1%}\")\n",
    "print(f\"   - Precision: {results[best_model_name]['precision']:.1%}\")\n",
    "print(f\"   - Recall: {results[best_model_name]['recall']:.1%}\")\n",
    "print(f\"   - F1-Score: {results[best_model_name]['f1']:.3f}\")\n",
    "\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    print(\"\\n4. MOST IMPORTANT FEATURES:\")\n",
    "    for idx, row in importance_df.head(3).iterrows():\n",
    "        print(f\"   - {row['Feature']}: {row['Importance']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "output_file = os.path.join(DATASETS_DIR, 'processed_sessions.csv')\n",
    "df_raw.to_csv(output_file, index=False)\n",
    "print(f\"Processed dataset saved to: {output_file}\")\n",
    "\n",
    "# Save model performance results\n",
    "results_file = os.path.join(DATASETS_DIR, 'model_results.csv')\n",
    "comparison_df.to_csv(results_file, index=False)\n",
    "print(f\"Model results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## 9. Conclusions\n",
    "\n",
    "### Summary\n",
    "\n",
    "This project successfully analyzed user interaction patterns with ArchieAI and developed predictive models for engagement classification. The analysis revealed:\n",
    "\n",
    "1. **Usage Patterns:** Students primarily use ArchieAI during afternoon and evening hours, with peak activity on weekdays.\n",
    "\n",
    "2. **Engagement Levels:** The dataset shows a distribution across high, medium, and low engagement levels, with clear distinctions in session duration and message counts.\n",
    "\n",
    "3. **Predictive Modeling:** Machine learning models successfully predict user engagement levels with reasonable accuracy. The best-performing model achieved [X]% accuracy on the test set.\n",
    "\n",
    "4. **Key Features:** Message count and time-based features are significant predictors of engagement level.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "Based on these findings, the following improvements are recommended for ArchieAI:\n",
    "\n",
    "1. **Optimize Availability:** Ensure maximum system availability during peak usage hours (afternoons and evenings).\n",
    "\n",
    "2. **Engagement Strategies:** Implement features to increase engagement for low-engagement users, such as:\n",
    "   - Suggested follow-up questions\n",
    "   - More interactive responses\n",
    "   - Personalized greetings for returning users\n",
    "\n",
    "3. **User Experience:** Consider session duration patterns when designing timeout and session management features.\n",
    "\n",
    "4. **Future Analysis:** Collect additional data on:\n",
    "   - Question categories/topics\n",
    "   - User satisfaction ratings\n",
    "   - Academic outcomes correlation\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Limited sample size may affect generalizability\n",
    "- Engagement classification is based on proxy metrics (duration, message count)\n",
    "- No direct user feedback data available\n",
    "- Analysis covers a limited time period\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Potential directions for extending this research:\n",
    "\n",
    "1. Implement natural language processing to analyze question content\n",
    "2. Develop a recommendation system for personalized AI responses\n",
    "3. Conduct A/B testing to validate engagement optimization strategies\n",
    "4. Integrate user feedback mechanisms and analyze satisfaction metrics\n",
    "5. Explore deep learning models for more nuanced engagement prediction\n",
    "\n",
    "---\n",
    "\n",
    "**Project completed successfully!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}